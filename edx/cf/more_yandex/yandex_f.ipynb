{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.fast_tensor_util' has no attribute 'AppendFloat32ArrayToTensorProto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Bring in subpackages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mINFINITE\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mINFINITE_CARDINALITY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/data/experimental/service/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMasterServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkerServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoShardPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExternalStatePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sparse_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections_abc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_collections_abc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m           \u001b[0m_MediumAppendFloat16ArrayToTensorProto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m           \u001b[0mfast_tensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAppendFloat32ArrayToTensorProto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m           \u001b[0mfast_tensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAppendFloat64ArrayToTensorProto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.fast_tensor_util' has no attribute 'AppendFloat32ArrayToTensorProto'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/vadim/anaconda3/envs/fastai/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'example'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6b92845b6f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this actually does make it a lot slower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/librosa/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;31m# Exporting data loader at the top level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;31m# Exporting all core functions is okay here: suppress the import warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'example'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "from pydub import AudioSegment\n",
    "# from pydub.playback import play\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "import functools\n",
    "import librosa, librosa.display\n",
    "print = functools.partial(print, flush=True) #this actually does make it a lot slower\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5d1f7e43366513a1d0a6ec5640c3dc24\\t1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9a701a4536a05b6610a590a9fe702ed8\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cad0b8547008d1524c1a0e5fd51f9908\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4bbe607e7dc95460e2cc1a6ee5f4dfa6\\t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30fb32cba90b34af26f3f14f5d636805\\t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fa33445afe71a6dc18e4881c053da5be\\t0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   5d1f7e43366513a1d0a6ec5640c3dc24\\t1\n",
       "0  9a701a4536a05b6610a590a9fe702ed8\\t1\n",
       "1  cad0b8547008d1524c1a0e5fd51f9908\\t1\n",
       "2  4bbe607e7dc95460e2cc1a6ee5f4dfa6\\t0\n",
       "3  30fb32cba90b34af26f3f14f5d636805\\t0\n",
       "4  fa33445afe71a6dc18e4881c053da5be\\t0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df = pd.read_csv(\"train/targets.tsv\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa, librosa.display\n",
    "import math\n",
    "from pydub import AudioSegment\n",
    "# from pydub.playback import play\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "import functools\n",
    "print = functools.partial(print, flush=True) #this actually does make it a lot slower\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "# os.chdir(dir_path)\n",
    "\n",
    "\n",
    "# signal, sr = librosa.load(file_path, sr=SAMPLE_RATE) # this is the soundfile method, which doesnt work with mp3/webm\n",
    "#check the samplerates = all webm youtube downloads have 48khz\n",
    "#pydub can open mp3/webm files and convert to samples/numpy arrays. thank you pydub\n",
    "#takes about 14 seconds to open 1hr long webm file, optimal way would be to just open chunks/buffers of it but oh well\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    #create accuracy subplot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"validation accuracy\")\n",
    "    axs[0].set_ylabel(\"accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "\n",
    "    #create loss subplot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train loss\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "    axs[1].set_ylabel(\"loss\")\n",
    "    axs[1].set_ylabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Loss\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mfcc_counter = 0\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "mlt = 2 #multiplier (yeah its weird)\n",
    "chunk = 1e3 * mlt  #1 second # 1e3 = 1000\n",
    "\n",
    "\n",
    "if 'mfcc_features.npy' in os.listdir():\n",
    "    print('loading saved data')\n",
    "    mfcc_features = load('mfcc_features.npy')\n",
    "    mfcc_labels = load('mfcc_labels.npy')\n",
    "else:\n",
    "\n",
    "    mfcc_features = np.empty((0,94*mlt,13))\n",
    "    mfcc_labels = []\n",
    "\n",
    "    for i, (root, dirs, filenames) in enumerate(os.walk(dir_path)):\n",
    "        for name in filenames:\n",
    "            if name.endswith('.webm'):\n",
    "\n",
    "                file_path = os.path.join(root, name)\n",
    "                print(file_path, i, file_path.split('\\\\')[-2])\n",
    "                sound = AudioSegment.from_file(file_path)\n",
    "                sound = sound.set_channels(1)\n",
    "                lenght_audio = len(sound)\n",
    "\n",
    "                number_mfccs = math.floor(lenght_audio/chunk)\n",
    "                print(lenght_audio/6e4,number_mfccs) #length in min, number chunks\n",
    "\n",
    "                mfcc_features_add = np.empty((number_mfccs,94*mlt,13))\n",
    "                mfcc_features = np.append(mfcc_features, mfcc_features_add, axis=0)\n",
    "\n",
    "                print(mfcc_features_add.shape)\n",
    "                print(mfcc_features.shape)\n",
    "\n",
    "                for x in range(number_mfccs):\n",
    "                    cut = sound[(x*chunk):(x*chunk)+chunk]\n",
    "                    samples = cut.get_array_of_samples()\n",
    "                    samples = np.array(samples)\n",
    "                    samples = samples.astype(float)\n",
    "\n",
    "                    # librosa.display.waveplot(samples, sr=sound.frame_rate)\n",
    "                    # plt.xlabel('Time')\n",
    "                    # plt.ylabel('Amplitude')\n",
    "                    # plt.show()\n",
    "\n",
    "                    mfcc = librosa.feature.mfcc(samples, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
    "                    mfcc = mfcc.T\n",
    "                    # librosa.display.specshow(mfcc, sr=44100, hop_length=hop_length)\n",
    "                    # plt.show()\n",
    "\n",
    "\n",
    "                    mfcc_features[mfcc_counter] = mfcc\n",
    "                    mfcc_labels.append(i-1)\n",
    "\n",
    "                    mfcc_counter += 1 \n",
    "\n",
    "                sound = None\n",
    "\n",
    "    mfcc_features = np.expand_dims(mfcc_features, axis=3)\n",
    "    save('mfcc_features.npy', mfcc_features)\n",
    "    mfcc_labels = np.array(mfcc_labels)\n",
    "    save('mfcc_labels.npy', mfcc_labels)\n",
    "\n",
    "print(mfcc_features.shape)\n",
    "\n",
    "output_neurons = mfcc_labels[-1] + 1\n",
    "print('output_neurons: ',output_neurons)\n",
    "\n",
    "\n",
    "#normalize??\n",
    "mean = np.mean(mfcc_features, axis=0)\n",
    "std = np.std(mfcc_features, axis=0)\n",
    "mfcc_features = (mfcc_features - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "#shuffle dataset ONCE\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(mfcc_features)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(mfcc_labels)\n",
    "np.random.seed()\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(mfcc_labels) * 0.9)\n",
    "val_size = len(mfcc_labels) - train_size\n",
    "\n",
    "print(train_size)\n",
    "print(val_size)\n",
    "\n",
    "training_ds = tf.data.Dataset.from_tensor_slices((mfcc_features,mfcc_labels))\n",
    "# training_ds = training_ds.shuffle(train_size + val_size)\n",
    "\n",
    "\n",
    "val_ds = training_ds.skip(train_size).take(val_size)\n",
    "training_ds = training_ds.take(train_size)\n",
    "\n",
    "print(val_ds)\n",
    "print(training_ds)\n",
    "\n",
    "\n",
    "#these two do the same thing, neat\n",
    "# print(next(iter(training_ds.take(1)))[1])\n",
    "# for item in training_ds.take(1):\n",
    "# \tprint(item[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################build model##############\n",
    "\n",
    "# batch_size = np_samples.shape[0]\n",
    "batch_size = 32 * 1\n",
    "STEPS_PER_EPOCH = train_size//batch_size\n",
    "checkpoint_path = 'savedweights_1/chkp-{val_accuracy}.ckpt'\n",
    "\n",
    "def get_callbacks():\n",
    "  return [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10), #this will stop the training if there is no improvement since the last 10 epochs\n",
    "    # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_weights_only=True, period=1) #unmute if you want to save ur weights during training\n",
    "  ]\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "\n",
    "inputShape = (94*mlt, 13, 1)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "\n",
    "    #1st conv layer\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(94*mlt, 13, 1), kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    #2nd conv layer\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.001)), #?? does it not need input layers here?\n",
    "    tf.keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    # #3rd conv layer\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPool2D((2,2), strides=(2,2), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    #flatten & dense & output\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(output_neurons)\n",
    "    ])\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              # loss=loss_fn,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# weight_path = '../savedweights_1/chkp-0.9171686768531799.ckpt'   \n",
    "# model.load_weights(weight_path)  #load them and comments out model.fit if u just want to use pre-existing weights\n",
    "\n",
    "# train\n",
    "history = model.fit(training_ds.batch(batch_size), \t\n",
    "    epochs=40, \n",
    "    validation_data=val_ds.shuffle(val_size).batch(batch_size),\n",
    "    callbacks=get_callbacks(),\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "\n",
    "model.evaluate(val_ds.batch(batch_size), verbose=2)\n",
    "\n",
    "\n",
    "#plot accuracy and error over the epochs\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "\n",
    "test_file = 'skrillex scary.webm'\n",
    "sound = AudioSegment.from_file(test_file)\n",
    "sound = sound.set_channels(1)\n",
    "lenght_audio = len(sound)\n",
    "\n",
    "number_mfccs = math.floor(lenght_audio/chunk)\n",
    "print(lenght_audio/6e4,number_mfccs) #length in min, number chunks\n",
    "\n",
    "test_mfccs = np.empty((number_mfccs,94*mlt,13))\n",
    "\n",
    "for x in range(number_mfccs):\n",
    "    cut = sound[(x*chunk):(x*chunk)+chunk]\n",
    "    samples = cut.get_array_of_samples()\n",
    "    samples = np.array(samples)\n",
    "    samples = samples.astype(float)\n",
    "    mfcc = librosa.feature.mfcc(samples, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
    "    mfcc = mfcc.T\n",
    "    # librosa.display.specshow(mfcc, sr=44100, hop_length=hop_length)\n",
    "    # plt.show()\n",
    "    test_mfccs[x] = mfcc\n",
    "\n",
    "# test_mfccs = np.expand_dims(test_mfccs, axis=0)\n",
    "test_mfccs = np.expand_dims(test_mfccs, axis=3)\n",
    "print(test_mfccs.shape)\n",
    "\n",
    "#normalize\n",
    "mean = np.mean(test_mfccs, axis=0)\n",
    "std = np.std(test_mfccs, axis=0)\n",
    "test_mfccs = (test_mfccs - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "class_names = ['ambient', 'classical', 'dubstep', 'hardstyle', 'jazz', 'trance']\n",
    "prob_results = []\n",
    "\n",
    "for i in range(len(test_mfccs)):\n",
    "    result = probability_model(test_mfccs[i:i+1]) #because it needs to be a list in a list, [i,4410] just returns a 1d list\n",
    "    answer = np.argmax(result[0])\n",
    "    prob_results.append(answer) \n",
    "    print(\"Skrillex is a \" + class_names[answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predict(X_test)\n",
    "\n",
    "df['tgender'] = yhat\n",
    "df.to_csv('targets.tsv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
