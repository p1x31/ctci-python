{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modern-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fantastic-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load various imports \n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subtle-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = \"/home/vadim/ctci-python/edx/cf/more_yandex/train\"\n",
    "filenames = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f.endswith('.wav'))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faced-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_id_in_file = [] # patient IDs corresponding to each file\n",
    "for name in filenames:\n",
    "    p_id_in_file.append(name[:-4])\n",
    "\n",
    "p_id_in_file = np.array(p_id_in_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distinguished-article",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['535df6bcbb6ddee2e21364b643fb7435' 'a8e2e7636226cb39ddac2b98d2a62606'\n",
      " '5dbbdcd354885a46f57d6330c68b63fe' ... '3a423fae5c860fe0348993fa4a14049d'\n",
      " '9fed420c4dded6e1ea8f264e730eff32' '83594e3ec9e164cbe5af64213f41fcbf']\n"
     ]
    }
   ],
   "source": [
    "print(p_id_in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "standing-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pad_len = 224 # to make the length of all MFCC equal\n",
    "\n",
    "def extract_features(file_name):\n",
    "    \"\"\"\n",
    "    This function takes in the path for an audio file as a string, loads it, and returns the MFCC\n",
    "    of the audio\"\"\"\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=5) \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=224)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "similar-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [join(mypath, f) for f in filenames] # full paths of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "freelance-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/vadim/ctci-python/edx/cf/more_yandex/train/targets.tsv\",sep='\\t', header=None) # patient diagnosis file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stunning-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d1f7e43366513a1d0a6ec5640c3dc24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9a701a4536a05b6610a590a9fe702ed8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cad0b8547008d1524c1a0e5fd51f9908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4bbe607e7dc95460e2cc1a6ee5f4dfa6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30fb32cba90b34af26f3f14f5d636805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0  1\n",
       "0  5d1f7e43366513a1d0a6ec5640c3dc24  1\n",
       "1  9a701a4536a05b6610a590a9fe702ed8  1\n",
       "2  cad0b8547008d1524c1a0e5fd51f9908  1\n",
       "3  4bbe607e7dc95460e2cc1a6ee5f4dfa6  0\n",
       "4  30fb32cba90b34af26f3f14f5d636805  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "treated-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[1].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stunning-shade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "13931    1\n",
       "13932    0\n",
       "13933    1\n",
       "13934    0\n",
       "13935    0\n",
       "Name: 1, Length: 13936, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "marine-controversy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  13936  files\n"
     ]
    }
   ],
   "source": [
    "features = [] \n",
    "\n",
    "# Iterate through each sound file and extract the features\n",
    "for file_name in filepaths:\n",
    "    data = extract_features(file_name)\n",
    "    features.append(data)\n",
    "\n",
    "print('Finished feature extraction from ', len(features), ' files')\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "confident-adjustment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(features[7], x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stretch-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features) # convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "shared-salvation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    1]\n",
      " [7620 6316]]\n"
     ]
    }
   ],
   "source": [
    "# print class counts\n",
    "unique_elements, counts_elements = np.unique(labels, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "static-valentine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot class counts\n",
    "y_pos = np.arange(len(unique_elements))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(unique_elements, counts_elements, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, unique_elements)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Gender')\n",
    "plt.title('Gender Count in sound files')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "another-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "le = LabelEncoder()\n",
    "i_labels = le.fit_transform(labels)\n",
    "oh_labels = to_categorical(i_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "decimal-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add channel dimension for CNN\n",
    "features = np.reshape(features, (*features.shape,3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "green-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, oh_labels, stratify=oh_labels, \n",
    "                                                    test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "canadian-excitement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11148"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "earned-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense, Input, Lambda, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img,ImageDataGenerator, img_to_array\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "applicable-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNet50V2(include_top=False, weights='imagenet',input_shape=(224,224,3))\n",
    "#base_model.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fewer-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "enormous-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "second-growth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50v2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_bn (BatchNo (None, 56, 56, 64)   256         pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_relu (Activ (None, 56, 56, 64)   0           conv2_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Add)          (None, 56, 56, 256)  0           conv2_block1_0_conv[0][0]        \n",
      "                                                                 conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 28, 28, 64)   36864       conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 28, 28, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 28, 28, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 28, 28, 256)  0           conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 28, 28, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Add)          (None, 28, 28, 256)  0           max_pooling2d_20[0][0]           \n",
      "                                                                 conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_bn (BatchNo (None, 28, 28, 256)  1024        conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_relu (Activ (None, 28, 28, 256)  0           conv3_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Add)          (None, 28, 28, 512)  0           conv3_block1_0_conv[0][0]        \n",
      "                                                                 conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 14, 14, 128)  147456      conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 14, 14, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 14, 14, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 14, 14, 512)  0           conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 14, 14, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Add)          (None, 14, 14, 512)  0           max_pooling2d_21[0][0]           \n",
      "                                                                 conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_bn (BatchNo (None, 14, 14, 512)  2048        conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_relu (Activ (None, 14, 14, 512)  0           conv4_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_conv[0][0]        \n",
      "                                                                 conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    589824      conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Add)          (None, 7, 7, 1024)   0           max_pooling2d_22[0][0]           \n",
      "                                                                 conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_bn (BatchNo (None, 7, 7, 1024)   4096        conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_relu (Activ (None, 7, 7, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_conv[0][0]        \n",
      "                                                                 conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "post_bn (BatchNormalization)    (None, 7, 7, 2048)   8192        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 7, 7, 2048)   0           post_bn[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,564,800\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,564,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "transsexual-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(128, (3, 3), activation='relu')(resnet_model.output)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100,activation='relu')(x)\n",
    "x = Dense(2,activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=resnet_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "reverse-incident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_bn (BatchNo (None, 56, 56, 64)   256         pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_relu (Activ (None, 56, 56, 64)   0           conv2_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Add)          (None, 56, 56, 256)  0           conv2_block1_0_conv[0][0]        \n",
      "                                                                 conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 28, 28, 64)   36864       conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 28, 28, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 28, 28, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 28, 28, 256)  0           conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 28, 28, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Add)          (None, 28, 28, 256)  0           max_pooling2d_20[0][0]           \n",
      "                                                                 conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_bn (BatchNo (None, 28, 28, 256)  1024        conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_relu (Activ (None, 28, 28, 256)  0           conv3_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Add)          (None, 28, 28, 512)  0           conv3_block1_0_conv[0][0]        \n",
      "                                                                 conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 14, 14, 128)  147456      conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 14, 14, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 14, 14, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 14, 14, 512)  0           conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 14, 14, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Add)          (None, 14, 14, 512)  0           max_pooling2d_21[0][0]           \n",
      "                                                                 conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_bn (BatchNo (None, 14, 14, 512)  2048        conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_relu (Activ (None, 14, 14, 512)  0           conv4_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_conv[0][0]        \n",
      "                                                                 conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    589824      conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Add)          (None, 7, 7, 1024)   0           max_pooling2d_22[0][0]           \n",
      "                                                                 conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_bn (BatchNo (None, 7, 7, 1024)   4096        conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_relu (Activ (None, 7, 7, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_conv[0][0]        \n",
      "                                                                 conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "post_bn (BatchNormalization)    (None, 7, 7, 2048)   8192        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 7, 7, 2048)   0           post_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 5, 5, 128)    2359424     post_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 2, 2, 128)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          51300       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            202         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 25,975,726\n",
      "Trainable params: 2,410,926\n",
      "Non-trainable params: 23,564,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train,\n",
    "                validation_data = testing_set,\n",
    "                epochs = 20,\n",
    "                steps_per_epoch=len(x_train),\n",
    "                validation_steps=len(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 40\n",
    "num_columns = 224\n",
    "num_channels = 3\n",
    "\n",
    "num_labels = oh_labels.shape[1]\n",
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "amended-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 40\n",
    "num_columns = 290\n",
    "num_channels = 1\n",
    "\n",
    "num_labels = oh_labels.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=filter_size,\n",
    "                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "threaded-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "legal-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 39, 289, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 144, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 19, 144, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 18, 143, 32)       2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 71, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 71, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 70, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 35, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 35, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 34, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 17, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 17, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.4656 - accuracy: 0.4534\n",
      "Pre-training accuracy: 45.3372%\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tender-lobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.1851 - accuracy: 0.5062\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.45481, saving model to mymodel2_01.h5\n",
      "88/88 [==============================] - 1s 17ms/step - loss: 1.1851 - accuracy: 0.5062 - val_loss: 0.7494 - val_accuracy: 0.4548\n",
      "Epoch 2/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.7390 - accuracy: 0.5135\n",
      "Epoch 00002: val_accuracy improved from 0.45481 to 0.47166, saving model to mymodel2_02.h5\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.7381 - accuracy: 0.5155 - val_loss: 0.6980 - val_accuracy: 0.4717\n",
      "Epoch 3/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.7109 - accuracy: 0.5180\n",
      "Epoch 00003: val_accuracy improved from 0.47166 to 0.50574, saving model to mymodel2_03.h5\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.7107 - accuracy: 0.5196 - val_loss: 0.6928 - val_accuracy: 0.5057\n",
      "Epoch 4/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.7060 - accuracy: 0.5142\n",
      "Epoch 00004: val_accuracy improved from 0.50574 to 0.52475, saving model to mymodel2_04.h5\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.7058 - accuracy: 0.5139 - val_loss: 0.6924 - val_accuracy: 0.5247\n",
      "Epoch 5/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6986 - accuracy: 0.5250\n",
      "Epoch 00005: val_accuracy improved from 0.52475 to 0.54699, saving model to mymodel2_05.h5\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6987 - accuracy: 0.5250 - val_loss: 0.6905 - val_accuracy: 0.5470\n",
      "Epoch 6/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.5289\n",
      "Epoch 00006: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6952 - accuracy: 0.5289 - val_loss: 0.6900 - val_accuracy: 0.5470\n",
      "Epoch 7/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6959 - accuracy: 0.5277\n",
      "Epoch 00007: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6960 - accuracy: 0.5275 - val_loss: 0.6896 - val_accuracy: 0.5466\n",
      "Epoch 8/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5359\n",
      "Epoch 00008: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6918 - accuracy: 0.5360 - val_loss: 0.6894 - val_accuracy: 0.5466\n",
      "Epoch 9/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6907 - accuracy: 0.5385\n",
      "Epoch 00009: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6906 - accuracy: 0.5386 - val_loss: 0.6900 - val_accuracy: 0.5466\n",
      "Epoch 10/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6896 - accuracy: 0.5458\n",
      "Epoch 00010: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6896 - accuracy: 0.5456 - val_loss: 0.6896 - val_accuracy: 0.5466\n",
      "Epoch 11/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6897 - accuracy: 0.5433\n",
      "Epoch 00011: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6898 - accuracy: 0.5432 - val_loss: 0.6897 - val_accuracy: 0.5466\n",
      "Epoch 12/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.5410\n",
      "Epoch 00012: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6909 - accuracy: 0.5413 - val_loss: 0.6894 - val_accuracy: 0.5466\n",
      "Epoch 13/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5451\n",
      "Epoch 00013: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6894 - accuracy: 0.5453 - val_loss: 0.6894 - val_accuracy: 0.5466\n",
      "Epoch 14/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6903 - accuracy: 0.5401\n",
      "Epoch 00014: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6900 - accuracy: 0.5414 - val_loss: 0.6893 - val_accuracy: 0.5466\n",
      "Epoch 15/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6899 - accuracy: 0.5473\n",
      "Epoch 00015: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6900 - accuracy: 0.5465 - val_loss: 0.6893 - val_accuracy: 0.5466\n",
      "Epoch 16/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5465\n",
      "Epoch 00016: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6890 - accuracy: 0.5461 - val_loss: 0.6891 - val_accuracy: 0.5466\n",
      "Epoch 17/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6896 - accuracy: 0.5449\n",
      "Epoch 00017: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6894 - accuracy: 0.5455 - val_loss: 0.6891 - val_accuracy: 0.5466\n",
      "Epoch 18/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.5459\n",
      "Epoch 00018: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6891 - accuracy: 0.5459 - val_loss: 0.6893 - val_accuracy: 0.5466\n",
      "Epoch 19/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5474\n",
      "Epoch 00019: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6893 - accuracy: 0.5475 - val_loss: 0.6891 - val_accuracy: 0.5466\n",
      "Epoch 20/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5453\n",
      "Epoch 00020: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6893 - accuracy: 0.5455 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 21/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5452\n",
      "Epoch 00021: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6889 - accuracy: 0.5457 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 22/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.5453\n",
      "Epoch 00022: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6899 - accuracy: 0.5453 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 23/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6889 - accuracy: 0.5465\n",
      "Epoch 00023: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6889 - accuracy: 0.5465 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 24/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5469\n",
      "Epoch 00024: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6893 - accuracy: 0.5469 - val_loss: 0.6891 - val_accuracy: 0.5466\n",
      "Epoch 25/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6894 - accuracy: 0.5461\n",
      "Epoch 00025: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6893 - accuracy: 0.5469 - val_loss: 0.6894 - val_accuracy: 0.5466\n",
      "Epoch 26/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6890 - accuracy: 0.5472\n",
      "Epoch 00026: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6890 - accuracy: 0.5471 - val_loss: 0.6892 - val_accuracy: 0.5466\n",
      "Epoch 27/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5475\n",
      "Epoch 00027: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6889 - accuracy: 0.5470 - val_loss: 0.6891 - val_accuracy: 0.5466\n",
      "Epoch 28/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5472\n",
      "Epoch 00028: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6889 - accuracy: 0.5471 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 29/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6892 - accuracy: 0.5459\n",
      "Epoch 00029: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6889 - accuracy: 0.5471 - val_loss: 0.6888 - val_accuracy: 0.5466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5459\n",
      "Epoch 00030: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6890 - accuracy: 0.5464 - val_loss: 0.6892 - val_accuracy: 0.5466\n",
      "Epoch 31/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5468\n",
      "Epoch 00031: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6888 - accuracy: 0.5468 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 32/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5471\n",
      "Epoch 00032: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6886 - accuracy: 0.5466 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 33/250\n",
      "83/88 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5467\n",
      "Epoch 00033: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6890 - accuracy: 0.5469 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 34/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5478\n",
      "Epoch 00034: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6889 - accuracy: 0.5469 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 35/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.5475\n",
      "Epoch 00035: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6887 - accuracy: 0.5469 - val_loss: 0.6892 - val_accuracy: 0.5466\n",
      "Epoch 36/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5447\n",
      "Epoch 00036: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6892 - accuracy: 0.5452 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 37/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5461\n",
      "Epoch 00037: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6888 - accuracy: 0.5468 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 38/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5457\n",
      "Epoch 00038: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6889 - accuracy: 0.5460 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 39/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6891 - accuracy: 0.5463\n",
      "Epoch 00039: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6890 - accuracy: 0.5469 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 40/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5466\n",
      "Epoch 00040: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5471 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 41/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5460\n",
      "Epoch 00041: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6888 - accuracy: 0.5468 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 42/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6887 - accuracy: 0.5467\n",
      "Epoch 00042: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6887 - accuracy: 0.5467 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 43/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6887 - accuracy: 0.5459\n",
      "Epoch 00043: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5457 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 44/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.5466\n",
      "Epoch 00044: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5466 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 45/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5461\n",
      "Epoch 00045: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6889 - accuracy: 0.5467 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 46/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5479\n",
      "Epoch 00046: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6889 - accuracy: 0.5481 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 47/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6887 - accuracy: 0.5472\n",
      "Epoch 00047: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5468 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 48/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5474\n",
      "Epoch 00048: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6888 - accuracy: 0.5473 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 49/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6887 - accuracy: 0.5475\n",
      "Epoch 00049: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6888 - accuracy: 0.5473 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 50/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.5466\n",
      "Epoch 00050: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6885 - accuracy: 0.5466 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 51/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.5472\n",
      "Epoch 00051: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5472 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 52/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5465\n",
      "Epoch 00052: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6890 - accuracy: 0.5459 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 53/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.5481\n",
      "Epoch 00053: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5468 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 54/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.5467\n",
      "Epoch 00054: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6884 - accuracy: 0.5467 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 55/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.5459\n",
      "Epoch 00055: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6890 - accuracy: 0.5459 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 56/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5485\n",
      "Epoch 00056: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5468 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 57/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5465\n",
      "Epoch 00057: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5466 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 58/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5463\n",
      "Epoch 00058: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5467 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 59/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5469\n",
      "Epoch 00059: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6890 - accuracy: 0.5468 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 60/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5492\n",
      "Epoch 00060: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5468 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 61/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.5470\n",
      "Epoch 00061: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6886 - accuracy: 0.5472 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 62/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6889 - accuracy: 0.5450\n",
      "Epoch 00062: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5467 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 63/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6887 - accuracy: 0.5467\n",
      "Epoch 00063: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5467 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 64/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6887 - accuracy: 0.5472\n",
      "Epoch 00064: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5468 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 65/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6890 - accuracy: 0.5460\n",
      "Epoch 00065: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5468 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 66/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.5472\n",
      "Epoch 00066: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6886 - accuracy: 0.5470 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 67/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6884 - accuracy: 0.5480\n",
      "Epoch 00067: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5470 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 68/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5466\n",
      "Epoch 00068: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6888 - accuracy: 0.5469 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 69/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5462\n",
      "Epoch 00069: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6887 - accuracy: 0.5468 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 70/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5472\n",
      "Epoch 00070: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6888 - accuracy: 0.5475 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 71/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6887 - accuracy: 0.5469\n",
      "Epoch 00071: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6887 - accuracy: 0.5467 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 72/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6888 - accuracy: 0.5440\n",
      "Epoch 00072: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6884 - accuracy: 0.5461 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 73/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6884 - accuracy: 0.5472\n",
      "Epoch 00073: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6885 - accuracy: 0.5472 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 74/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6887 - accuracy: 0.5454\n",
      "Epoch 00074: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5458 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 75/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6887 - accuracy: 0.5467\n",
      "Epoch 00075: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6886 - accuracy: 0.5466 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 76/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5462\n",
      "Epoch 00076: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6882 - accuracy: 0.5462 - val_loss: 0.6884 - val_accuracy: 0.5466\n",
      "Epoch 77/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5471\n",
      "Epoch 00077: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5476 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 78/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6887 - accuracy: 0.5477\n",
      "Epoch 00078: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6886 - accuracy: 0.5482 - val_loss: 0.6885 - val_accuracy: 0.5466\n",
      "Epoch 79/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.5465\n",
      "Epoch 00079: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6885 - accuracy: 0.5465 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 80/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6892 - accuracy: 0.5461\n",
      "Epoch 00080: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6890 - accuracy: 0.5470 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 81/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.5470\n",
      "Epoch 00081: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6886 - accuracy: 0.5467 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 82/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5454\n",
      "Epoch 00082: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6889 - accuracy: 0.5456 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 83/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5478\n",
      "Epoch 00083: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5466 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 84/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6887 - accuracy: 0.5468\n",
      "Epoch 00084: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6887 - accuracy: 0.5468 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 85/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6887 - accuracy: 0.5454\n",
      "Epoch 00085: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6885 - accuracy: 0.5468 - val_loss: 0.6888 - val_accuracy: 0.5466\n",
      "Epoch 86/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6886 - accuracy: 0.5487\n",
      "Epoch 00086: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5476 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 87/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5467\n",
      "Epoch 00087: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5468 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 88/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/88 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5462\n",
      "Epoch 00088: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5464 - val_loss: 0.6885 - val_accuracy: 0.5466\n",
      "Epoch 89/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5468\n",
      "Epoch 00089: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6883 - accuracy: 0.5470 - val_loss: 0.6884 - val_accuracy: 0.5466\n",
      "Epoch 90/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6887 - accuracy: 0.5471\n",
      "Epoch 00090: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5466 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 91/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5472\n",
      "Epoch 00091: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6885 - accuracy: 0.5471 - val_loss: 0.6885 - val_accuracy: 0.5466\n",
      "Epoch 92/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6882 - accuracy: 0.5473\n",
      "Epoch 00092: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6883 - accuracy: 0.5470 - val_loss: 0.6890 - val_accuracy: 0.5456\n",
      "Epoch 93/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6878 - accuracy: 0.5460\n",
      "Epoch 00093: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6879 - accuracy: 0.5459 - val_loss: 0.6885 - val_accuracy: 0.5463\n",
      "Epoch 94/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6884 - accuracy: 0.5475\n",
      "Epoch 00094: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6886 - accuracy: 0.5471 - val_loss: 0.6892 - val_accuracy: 0.5459\n",
      "Epoch 95/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.5464\n",
      "Epoch 00095: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6885 - accuracy: 0.5464 - val_loss: 0.6890 - val_accuracy: 0.5459\n",
      "Epoch 96/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5460\n",
      "Epoch 00096: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6882 - accuracy: 0.5459 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 97/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5444\n",
      "Epoch 00097: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6888 - accuracy: 0.5444 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 98/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5461\n",
      "Epoch 00098: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6885 - accuracy: 0.5479 - val_loss: 0.6891 - val_accuracy: 0.5456\n",
      "Epoch 99/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5477\n",
      "Epoch 00099: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6883 - accuracy: 0.5473 - val_loss: 0.6886 - val_accuracy: 0.5466\n",
      "Epoch 100/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6882 - accuracy: 0.5465\n",
      "Epoch 00100: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6880 - accuracy: 0.5469 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 101/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5464\n",
      "Epoch 00101: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6884 - accuracy: 0.5469 - val_loss: 0.6887 - val_accuracy: 0.5466\n",
      "Epoch 102/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.5478\n",
      "Epoch 00102: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6884 - accuracy: 0.5478 - val_loss: 0.6890 - val_accuracy: 0.5466\n",
      "Epoch 103/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5479\n",
      "Epoch 00103: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6882 - accuracy: 0.5482 - val_loss: 0.6891 - val_accuracy: 0.5466\n",
      "Epoch 104/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6882 - accuracy: 0.5479\n",
      "Epoch 00104: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6884 - accuracy: 0.5471 - val_loss: 0.6889 - val_accuracy: 0.5466\n",
      "Epoch 105/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5475\n",
      "Epoch 00105: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6882 - accuracy: 0.5473 - val_loss: 0.6887 - val_accuracy: 0.5456\n",
      "Epoch 106/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5453\n",
      "Epoch 00106: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6886 - accuracy: 0.5446 - val_loss: 0.6891 - val_accuracy: 0.5463\n",
      "Epoch 107/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6885 - accuracy: 0.5470\n",
      "Epoch 00107: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6884 - accuracy: 0.5474 - val_loss: 0.6892 - val_accuracy: 0.5466\n",
      "Epoch 108/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6884 - accuracy: 0.5465\n",
      "Epoch 00108: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6884 - accuracy: 0.5465 - val_loss: 0.6890 - val_accuracy: 0.5459\n",
      "Epoch 109/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6880 - accuracy: 0.5481\n",
      "Epoch 00109: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6882 - accuracy: 0.5470 - val_loss: 0.6884 - val_accuracy: 0.5463\n",
      "Epoch 110/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6877 - accuracy: 0.5482\n",
      "Epoch 00110: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6879 - accuracy: 0.5474 - val_loss: 0.6888 - val_accuracy: 0.5463\n",
      "Epoch 111/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5469\n",
      "Epoch 00111: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6880 - accuracy: 0.5473 - val_loss: 0.6889 - val_accuracy: 0.5463\n",
      "Epoch 112/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5465\n",
      "Epoch 00112: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6885 - accuracy: 0.5466 - val_loss: 0.6889 - val_accuracy: 0.5463\n",
      "Epoch 113/250\n",
      "83/88 [===========================>..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5467\n",
      "Epoch 00113: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6881 - accuracy: 0.5471 - val_loss: 0.6889 - val_accuracy: 0.5448\n",
      "Epoch 114/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5455\n",
      "Epoch 00114: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6880 - accuracy: 0.5462 - val_loss: 0.6889 - val_accuracy: 0.5459\n",
      "Epoch 115/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6877 - accuracy: 0.5472\n",
      "Epoch 00115: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6876 - accuracy: 0.5483 - val_loss: 0.6889 - val_accuracy: 0.5456\n",
      "Epoch 116/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6877 - accuracy: 0.5474\n",
      "Epoch 00116: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6876 - accuracy: 0.5478 - val_loss: 0.6894 - val_accuracy: 0.5470\n",
      "Epoch 117/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6875 - accuracy: 0.5504\n",
      "Epoch 00117: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6875 - accuracy: 0.5502 - val_loss: 0.6892 - val_accuracy: 0.5466\n",
      "Epoch 118/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.5466\n",
      "Epoch 00118: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6881 - accuracy: 0.5465 - val_loss: 0.6890 - val_accuracy: 0.5459\n",
      "Epoch 119/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6879 - accuracy: 0.5451\n",
      "Epoch 00119: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6878 - accuracy: 0.5457 - val_loss: 0.6889 - val_accuracy: 0.5459\n",
      "Epoch 120/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6878 - accuracy: 0.5464\n",
      "Epoch 00120: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6879 - accuracy: 0.5460 - val_loss: 0.6893 - val_accuracy: 0.5459\n",
      "Epoch 121/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.5463\n",
      "Epoch 00121: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6880 - accuracy: 0.5463 - val_loss: 0.6890 - val_accuracy: 0.5463\n",
      "Epoch 122/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6881 - accuracy: 0.5493\n",
      "Epoch 00122: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6878 - accuracy: 0.5501 - val_loss: 0.6889 - val_accuracy: 0.5459\n",
      "Epoch 123/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6879 - accuracy: 0.5469\n",
      "Epoch 00123: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6880 - accuracy: 0.5468 - val_loss: 0.6891 - val_accuracy: 0.5459\n",
      "Epoch 124/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6883 - accuracy: 0.5468\n",
      "Epoch 00124: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6881 - accuracy: 0.5480 - val_loss: 0.6891 - val_accuracy: 0.5452\n",
      "Epoch 125/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.5483\n",
      "Epoch 00125: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6877 - accuracy: 0.5491 - val_loss: 0.6892 - val_accuracy: 0.5445\n",
      "Epoch 126/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.5471\n",
      "Epoch 00126: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6879 - accuracy: 0.5467 - val_loss: 0.6891 - val_accuracy: 0.5448\n",
      "Epoch 127/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6872 - accuracy: 0.5471\n",
      "Epoch 00127: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6871 - accuracy: 0.5475 - val_loss: 0.6902 - val_accuracy: 0.5423\n",
      "Epoch 128/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6878 - accuracy: 0.5475\n",
      "Epoch 00128: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6878 - accuracy: 0.5468 - val_loss: 0.6894 - val_accuracy: 0.5423\n",
      "Epoch 129/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6870 - accuracy: 0.5493\n",
      "Epoch 00129: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6871 - accuracy: 0.5490 - val_loss: 0.6903 - val_accuracy: 0.5355\n",
      "Epoch 130/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6876 - accuracy: 0.5484\n",
      "Epoch 00130: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6876 - accuracy: 0.5484 - val_loss: 0.6891 - val_accuracy: 0.5456\n",
      "Epoch 131/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6873 - accuracy: 0.5502\n",
      "Epoch 00131: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6872 - accuracy: 0.5505 - val_loss: 0.6892 - val_accuracy: 0.5445\n",
      "Epoch 132/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6873 - accuracy: 0.5494\n",
      "Epoch 00132: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6871 - accuracy: 0.5496 - val_loss: 0.6889 - val_accuracy: 0.5463\n",
      "Epoch 133/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.5476\n",
      "Epoch 00133: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6871 - accuracy: 0.5475 - val_loss: 0.6894 - val_accuracy: 0.5445\n",
      "Epoch 134/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6875 - accuracy: 0.5471\n",
      "Epoch 00134: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6874 - accuracy: 0.5476 - val_loss: 0.6895 - val_accuracy: 0.5470\n",
      "Epoch 135/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6873 - accuracy: 0.5483\n",
      "Epoch 00135: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6873 - accuracy: 0.5483 - val_loss: 0.6892 - val_accuracy: 0.5434\n",
      "Epoch 136/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6876 - accuracy: 0.5440\n",
      "Epoch 00136: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6874 - accuracy: 0.5447 - val_loss: 0.6895 - val_accuracy: 0.5452\n",
      "Epoch 137/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6874 - accuracy: 0.5503\n",
      "Epoch 00137: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6877 - accuracy: 0.5490 - val_loss: 0.6892 - val_accuracy: 0.5445\n",
      "Epoch 138/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6870 - accuracy: 0.5530\n",
      "Epoch 00138: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6870 - accuracy: 0.5531 - val_loss: 0.6903 - val_accuracy: 0.5405\n",
      "Epoch 139/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6869 - accuracy: 0.5517\n",
      "Epoch 00139: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6871 - accuracy: 0.5506 - val_loss: 0.6891 - val_accuracy: 0.5459\n",
      "Epoch 140/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.5507\n",
      "Epoch 00140: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6866 - accuracy: 0.5507 - val_loss: 0.6900 - val_accuracy: 0.5384\n",
      "Epoch 141/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5528\n",
      "Epoch 00141: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6867 - accuracy: 0.5525 - val_loss: 0.6915 - val_accuracy: 0.5326\n",
      "Epoch 142/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.5476\n",
      "Epoch 00142: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6868 - accuracy: 0.5484 - val_loss: 0.6897 - val_accuracy: 0.5441\n",
      "Epoch 143/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.5486\n",
      "Epoch 00143: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6871 - accuracy: 0.5482 - val_loss: 0.6893 - val_accuracy: 0.5430\n",
      "Epoch 144/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.5524\n",
      "Epoch 00144: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6862 - accuracy: 0.5524 - val_loss: 0.6901 - val_accuracy: 0.5430\n",
      "Epoch 145/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6875 - accuracy: 0.5486\n",
      "Epoch 00145: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6873 - accuracy: 0.5497 - val_loss: 0.6902 - val_accuracy: 0.5441\n",
      "Epoch 146/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6859 - accuracy: 0.5545\n",
      "Epoch 00146: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6861 - accuracy: 0.5540 - val_loss: 0.6900 - val_accuracy: 0.5452\n",
      "Epoch 147/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6868 - accuracy: 0.5513\n",
      "Epoch 00147: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6866 - accuracy: 0.5519 - val_loss: 0.6893 - val_accuracy: 0.5448\n",
      "Epoch 148/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5489\n",
      "Epoch 00148: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6864 - accuracy: 0.5497 - val_loss: 0.6894 - val_accuracy: 0.5405\n",
      "Epoch 149/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.5536\n",
      "Epoch 00149: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6856 - accuracy: 0.5527 - val_loss: 0.6902 - val_accuracy: 0.5405\n",
      "Epoch 150/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5510\n",
      "Epoch 00150: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6862 - accuracy: 0.5508 - val_loss: 0.6901 - val_accuracy: 0.5434\n",
      "Epoch 151/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6862 - accuracy: 0.5514\n",
      "Epoch 00151: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6864 - accuracy: 0.5510 - val_loss: 0.6898 - val_accuracy: 0.5441\n",
      "Epoch 152/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.5528\n",
      "Epoch 00152: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6857 - accuracy: 0.5527 - val_loss: 0.6899 - val_accuracy: 0.5427\n",
      "Epoch 153/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6854 - accuracy: 0.5526\n",
      "Epoch 00153: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6854 - accuracy: 0.5525 - val_loss: 0.6906 - val_accuracy: 0.5398\n",
      "Epoch 154/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6860 - accuracy: 0.5505\n",
      "Epoch 00154: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6860 - accuracy: 0.5510 - val_loss: 0.6906 - val_accuracy: 0.5412\n",
      "Epoch 155/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.5516\n",
      "Epoch 00155: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6858 - accuracy: 0.5516 - val_loss: 0.6908 - val_accuracy: 0.5434\n",
      "Epoch 156/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6850 - accuracy: 0.5556\n",
      "Epoch 00156: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6849 - accuracy: 0.5564 - val_loss: 0.6905 - val_accuracy: 0.5441\n",
      "Epoch 157/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6858 - accuracy: 0.5520\n",
      "Epoch 00157: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6857 - accuracy: 0.5526 - val_loss: 0.6902 - val_accuracy: 0.5448\n",
      "Epoch 158/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5553\n",
      "Epoch 00158: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6854 - accuracy: 0.5548 - val_loss: 0.6907 - val_accuracy: 0.5448\n",
      "Epoch 159/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5511\n",
      "Epoch 00159: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6864 - accuracy: 0.5503 - val_loss: 0.6908 - val_accuracy: 0.5438\n",
      "Epoch 160/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.5522\n",
      "Epoch 00160: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6858 - accuracy: 0.5522 - val_loss: 0.6902 - val_accuracy: 0.5452\n",
      "Epoch 161/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5541\n",
      "Epoch 00161: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6854 - accuracy: 0.5540 - val_loss: 0.6910 - val_accuracy: 0.5409\n",
      "Epoch 162/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6855 - accuracy: 0.5568\n",
      "Epoch 00162: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6855 - accuracy: 0.5568 - val_loss: 0.6904 - val_accuracy: 0.5452\n",
      "Epoch 163/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6853 - accuracy: 0.5534\n",
      "Epoch 00163: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6852 - accuracy: 0.5540 - val_loss: 0.6911 - val_accuracy: 0.5448\n",
      "Epoch 164/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5545\n",
      "Epoch 00164: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6851 - accuracy: 0.5540 - val_loss: 0.6918 - val_accuracy: 0.5466\n",
      "Epoch 165/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.5526\n",
      "Epoch 00165: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6854 - accuracy: 0.5526 - val_loss: 0.6903 - val_accuracy: 0.5456\n",
      "Epoch 166/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6834 - accuracy: 0.5513\n",
      "Epoch 00166: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6838 - accuracy: 0.5512 - val_loss: 0.6925 - val_accuracy: 0.5420\n",
      "Epoch 167/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5531\n",
      "Epoch 00167: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6845 - accuracy: 0.5544 - val_loss: 0.6931 - val_accuracy: 0.5387\n",
      "Epoch 168/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6845 - accuracy: 0.5530\n",
      "Epoch 00168: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6844 - accuracy: 0.5535 - val_loss: 0.6916 - val_accuracy: 0.5448\n",
      "Epoch 169/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6844 - accuracy: 0.5560\n",
      "Epoch 00169: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6844 - accuracy: 0.5559 - val_loss: 0.6917 - val_accuracy: 0.5448\n",
      "Epoch 170/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.5551\n",
      "Epoch 00170: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6843 - accuracy: 0.5551 - val_loss: 0.6931 - val_accuracy: 0.5366\n",
      "Epoch 171/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.5505\n",
      "Epoch 00171: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6853 - accuracy: 0.5505 - val_loss: 0.6905 - val_accuracy: 0.5434\n",
      "Epoch 172/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5514\n",
      "Epoch 00172: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6848 - accuracy: 0.5514 - val_loss: 0.6916 - val_accuracy: 0.5380\n",
      "Epoch 173/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6834 - accuracy: 0.5552\n",
      "Epoch 00173: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6835 - accuracy: 0.5549 - val_loss: 0.6920 - val_accuracy: 0.5362\n",
      "Epoch 174/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5544\n",
      "Epoch 00174: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6833 - accuracy: 0.5543 - val_loss: 0.6921 - val_accuracy: 0.5391\n",
      "Epoch 175/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6832 - accuracy: 0.5580\n",
      "Epoch 00175: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6833 - accuracy: 0.5568 - val_loss: 0.6902 - val_accuracy: 0.5448\n",
      "Epoch 176/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6839 - accuracy: 0.5556\n",
      "Epoch 00176: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6840 - accuracy: 0.5558 - val_loss: 0.6927 - val_accuracy: 0.5312\n",
      "Epoch 177/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5512\n",
      "Epoch 00177: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6851 - accuracy: 0.5510 - val_loss: 0.6917 - val_accuracy: 0.5430\n",
      "Epoch 178/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6826 - accuracy: 0.5568\n",
      "Epoch 00178: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6826 - accuracy: 0.5568 - val_loss: 0.6922 - val_accuracy: 0.5369\n",
      "Epoch 179/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6824 - accuracy: 0.5619\n",
      "Epoch 00179: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6825 - accuracy: 0.5611 - val_loss: 0.6918 - val_accuracy: 0.5427\n",
      "Epoch 180/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6830 - accuracy: 0.5594\n",
      "Epoch 00180: val_accuracy did not improve from 0.54699\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6830 - accuracy: 0.5595 - val_loss: 0.6924 - val_accuracy: 0.5319\n",
      "Epoch 181/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6835 - accuracy: 0.5580\n",
      "Epoch 00181: val_accuracy improved from 0.54699 to 0.54735, saving model to mymodel2_181.h5\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6835 - accuracy: 0.5580 - val_loss: 0.6921 - val_accuracy: 0.5473\n",
      "Epoch 182/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6833 - accuracy: 0.5585\n",
      "Epoch 00182: val_accuracy did not improve from 0.54735\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6833 - accuracy: 0.5584 - val_loss: 0.6920 - val_accuracy: 0.5423\n",
      "Epoch 183/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.5565\n",
      "Epoch 00183: val_accuracy improved from 0.54735 to 0.54806, saving model to mymodel2_183.h5\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6836 - accuracy: 0.5567 - val_loss: 0.6925 - val_accuracy: 0.5481\n",
      "Epoch 184/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6840 - accuracy: 0.5573\n",
      "Epoch 00184: val_accuracy did not improve from 0.54806\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6840 - accuracy: 0.5574 - val_loss: 0.6924 - val_accuracy: 0.5380\n",
      "Epoch 185/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6823 - accuracy: 0.5592\n",
      "Epoch 00185: val_accuracy did not improve from 0.54806\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6823 - accuracy: 0.5597 - val_loss: 0.6927 - val_accuracy: 0.5341\n",
      "Epoch 186/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.5562\n",
      "Epoch 00186: val_accuracy did not improve from 0.54806\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6833 - accuracy: 0.5562 - val_loss: 0.6925 - val_accuracy: 0.5466\n",
      "Epoch 187/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6834 - accuracy: 0.5544\n",
      "Epoch 00187: val_accuracy did not improve from 0.54806\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6833 - accuracy: 0.5547 - val_loss: 0.6921 - val_accuracy: 0.5463\n",
      "Epoch 188/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6816 - accuracy: 0.5598\n",
      "Epoch 00188: val_accuracy did not improve from 0.54806\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6816 - accuracy: 0.5605 - val_loss: 0.6942 - val_accuracy: 0.5330\n",
      "Epoch 189/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6833 - accuracy: 0.5585\n",
      "Epoch 00189: val_accuracy did not improve from 0.54806\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6831 - accuracy: 0.5592 - val_loss: 0.6932 - val_accuracy: 0.5427\n",
      "Epoch 190/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6825 - accuracy: 0.5628\n",
      "Epoch 00190: val_accuracy did not improve from 0.54806\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6822 - accuracy: 0.5630 - val_loss: 0.6951 - val_accuracy: 0.5305\n",
      "Epoch 191/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6839 - accuracy: 0.5563\n",
      "Epoch 00191: val_accuracy improved from 0.54806 to 0.54878, saving model to mymodel2_191.h5\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6838 - accuracy: 0.5570 - val_loss: 0.6925 - val_accuracy: 0.5488\n",
      "Epoch 192/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6818 - accuracy: 0.5651\n",
      "Epoch 00192: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6822 - accuracy: 0.5632 - val_loss: 0.6936 - val_accuracy: 0.5301\n",
      "Epoch 193/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6834 - accuracy: 0.5574\n",
      "Epoch 00193: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6836 - accuracy: 0.5572 - val_loss: 0.6930 - val_accuracy: 0.5387\n",
      "Epoch 194/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6826 - accuracy: 0.5604\n",
      "Epoch 00194: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6825 - accuracy: 0.5598 - val_loss: 0.6920 - val_accuracy: 0.5430\n",
      "Epoch 195/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6809 - accuracy: 0.5589\n",
      "Epoch 00195: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6811 - accuracy: 0.5583 - val_loss: 0.6927 - val_accuracy: 0.5470\n",
      "Epoch 196/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5588\n",
      "Epoch 00196: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6809 - accuracy: 0.5582 - val_loss: 0.6920 - val_accuracy: 0.5359\n",
      "Epoch 197/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6841 - accuracy: 0.5557\n",
      "Epoch 00197: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6839 - accuracy: 0.5562 - val_loss: 0.6916 - val_accuracy: 0.5445\n",
      "Epoch 198/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5575\n",
      "Epoch 00198: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6812 - accuracy: 0.5587 - val_loss: 0.6915 - val_accuracy: 0.5466\n",
      "Epoch 199/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6803 - accuracy: 0.5615\n",
      "Epoch 00199: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6803 - accuracy: 0.5617 - val_loss: 0.6936 - val_accuracy: 0.5362\n",
      "Epoch 200/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6813 - accuracy: 0.5661\n",
      "Epoch 00200: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6814 - accuracy: 0.5656 - val_loss: 0.6916 - val_accuracy: 0.5445\n",
      "Epoch 201/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6790 - accuracy: 0.5640\n",
      "Epoch 00201: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6791 - accuracy: 0.5637 - val_loss: 0.6916 - val_accuracy: 0.5405\n",
      "Epoch 202/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6782 - accuracy: 0.5700\n",
      "Epoch 00202: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6785 - accuracy: 0.5689 - val_loss: 0.6924 - val_accuracy: 0.5377\n",
      "Epoch 203/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5574\n",
      "Epoch 00203: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6811 - accuracy: 0.5573 - val_loss: 0.6941 - val_accuracy: 0.5276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.5609\n",
      "Epoch 00204: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6809 - accuracy: 0.5609 - val_loss: 0.6925 - val_accuracy: 0.5380\n",
      "Epoch 205/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6800 - accuracy: 0.5629\n",
      "Epoch 00205: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6802 - accuracy: 0.5630 - val_loss: 0.6943 - val_accuracy: 0.5312\n",
      "Epoch 206/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5592\n",
      "Epoch 00206: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 0.6817 - accuracy: 0.5585 - val_loss: 0.6934 - val_accuracy: 0.5323\n",
      "Epoch 207/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6804 - accuracy: 0.5638\n",
      "Epoch 00207: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6807 - accuracy: 0.5632 - val_loss: 0.6922 - val_accuracy: 0.5384\n",
      "Epoch 208/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.5641\n",
      "Epoch 00208: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6793 - accuracy: 0.5641 - val_loss: 0.6942 - val_accuracy: 0.5380\n",
      "Epoch 209/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6813 - accuracy: 0.5636\n",
      "Epoch 00209: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6812 - accuracy: 0.5631 - val_loss: 0.6925 - val_accuracy: 0.5441\n",
      "Epoch 210/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6808 - accuracy: 0.5655\n",
      "Epoch 00210: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 16ms/step - loss: 0.6805 - accuracy: 0.5660 - val_loss: 0.6941 - val_accuracy: 0.5319\n",
      "Epoch 211/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6799 - accuracy: 0.5629\n",
      "Epoch 00211: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6798 - accuracy: 0.5629 - val_loss: 0.6922 - val_accuracy: 0.5359\n",
      "Epoch 212/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6794 - accuracy: 0.5642\n",
      "Epoch 00212: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6794 - accuracy: 0.5642 - val_loss: 0.6932 - val_accuracy: 0.5316\n",
      "Epoch 213/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6794 - accuracy: 0.5648\n",
      "Epoch 00213: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6791 - accuracy: 0.5655 - val_loss: 0.6927 - val_accuracy: 0.5369\n",
      "Epoch 214/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6796 - accuracy: 0.5652\n",
      "Epoch 00214: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6792 - accuracy: 0.5659 - val_loss: 0.6941 - val_accuracy: 0.5233\n",
      "Epoch 215/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6767 - accuracy: 0.5678\n",
      "Epoch 00215: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6764 - accuracy: 0.5690 - val_loss: 0.6944 - val_accuracy: 0.5452\n",
      "Epoch 216/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5642\n",
      "Epoch 00216: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6810 - accuracy: 0.5640 - val_loss: 0.6928 - val_accuracy: 0.5391\n",
      "Epoch 217/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5596\n",
      "Epoch 00217: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6791 - accuracy: 0.5596 - val_loss: 0.6933 - val_accuracy: 0.5366\n",
      "Epoch 218/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6796 - accuracy: 0.5610\n",
      "Epoch 00218: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6796 - accuracy: 0.5611 - val_loss: 0.6927 - val_accuracy: 0.5380\n",
      "Epoch 219/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6785 - accuracy: 0.5687\n",
      "Epoch 00219: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6784 - accuracy: 0.5693 - val_loss: 0.6938 - val_accuracy: 0.5409\n",
      "Epoch 220/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6802 - accuracy: 0.5636\n",
      "Epoch 00220: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6800 - accuracy: 0.5639 - val_loss: 0.6936 - val_accuracy: 0.5269\n",
      "Epoch 221/250\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5622\n",
      "Epoch 00221: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6802 - accuracy: 0.5620 - val_loss: 0.6938 - val_accuracy: 0.5344\n",
      "Epoch 222/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6790 - accuracy: 0.5655\n",
      "Epoch 00222: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6790 - accuracy: 0.5652 - val_loss: 0.6934 - val_accuracy: 0.5398\n",
      "Epoch 223/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6784 - accuracy: 0.5677\n",
      "Epoch 00223: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6787 - accuracy: 0.5675 - val_loss: 0.6935 - val_accuracy: 0.5387\n",
      "Epoch 224/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6780 - accuracy: 0.5670\n",
      "Epoch 00224: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6781 - accuracy: 0.5664 - val_loss: 0.6966 - val_accuracy: 0.5298\n",
      "Epoch 225/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6774 - accuracy: 0.5668\n",
      "Epoch 00225: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6777 - accuracy: 0.5658 - val_loss: 0.6939 - val_accuracy: 0.5369\n",
      "Epoch 226/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6790 - accuracy: 0.5660\n",
      "Epoch 00226: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6791 - accuracy: 0.5654 - val_loss: 0.6934 - val_accuracy: 0.5466\n",
      "Epoch 227/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6779 - accuracy: 0.5689\n",
      "Epoch 00227: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6781 - accuracy: 0.5684 - val_loss: 0.6927 - val_accuracy: 0.5423\n",
      "Epoch 228/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.5713\n",
      "Epoch 00228: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6774 - accuracy: 0.5713 - val_loss: 0.6949 - val_accuracy: 0.5387\n",
      "Epoch 229/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6776 - accuracy: 0.5684\n",
      "Epoch 00229: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6777 - accuracy: 0.5678 - val_loss: 0.6930 - val_accuracy: 0.5352\n",
      "Epoch 230/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6751 - accuracy: 0.5741\n",
      "Epoch 00230: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6754 - accuracy: 0.5734 - val_loss: 0.6943 - val_accuracy: 0.5377\n",
      "Epoch 231/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.5734\n",
      "Epoch 00231: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6763 - accuracy: 0.5735 - val_loss: 0.6932 - val_accuracy: 0.5373\n",
      "Epoch 232/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6769 - accuracy: 0.5709\n",
      "Epoch 00232: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6766 - accuracy: 0.5715 - val_loss: 0.6941 - val_accuracy: 0.5387\n",
      "Epoch 233/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/88 [============================>.] - ETA: 0s - loss: 0.6780 - accuracy: 0.5664\n",
      "Epoch 00233: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6781 - accuracy: 0.5666 - val_loss: 0.6920 - val_accuracy: 0.5366\n",
      "Epoch 234/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6758 - accuracy: 0.5671\n",
      "Epoch 00234: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6757 - accuracy: 0.5670 - val_loss: 0.6945 - val_accuracy: 0.5398\n",
      "Epoch 235/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6756 - accuracy: 0.5755\n",
      "Epoch 00235: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6755 - accuracy: 0.5757 - val_loss: 0.6944 - val_accuracy: 0.5326\n",
      "Epoch 236/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6764 - accuracy: 0.5742\n",
      "Epoch 00236: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6766 - accuracy: 0.5728 - val_loss: 0.6950 - val_accuracy: 0.5395\n",
      "Epoch 237/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6776 - accuracy: 0.5703\n",
      "Epoch 00237: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6773 - accuracy: 0.5710 - val_loss: 0.6934 - val_accuracy: 0.5362\n",
      "Epoch 238/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6765 - accuracy: 0.5685\n",
      "Epoch 00238: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6765 - accuracy: 0.5677 - val_loss: 0.6928 - val_accuracy: 0.5409\n",
      "Epoch 239/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6764 - accuracy: 0.5670\n",
      "Epoch 00239: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6763 - accuracy: 0.5673 - val_loss: 0.6949 - val_accuracy: 0.5341\n",
      "Epoch 240/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6772 - accuracy: 0.5705\n",
      "Epoch 00240: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6772 - accuracy: 0.5705 - val_loss: 0.6948 - val_accuracy: 0.5409\n",
      "Epoch 241/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6747 - accuracy: 0.5763\n",
      "Epoch 00241: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6746 - accuracy: 0.5763 - val_loss: 0.6947 - val_accuracy: 0.5323\n",
      "Epoch 242/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6774 - accuracy: 0.5692\n",
      "Epoch 00242: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 16ms/step - loss: 0.6773 - accuracy: 0.5697 - val_loss: 0.6920 - val_accuracy: 0.5445\n",
      "Epoch 243/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6730 - accuracy: 0.5740\n",
      "Epoch 00243: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6728 - accuracy: 0.5742 - val_loss: 0.6966 - val_accuracy: 0.5348\n",
      "Epoch 244/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6763 - accuracy: 0.5710\n",
      "Epoch 00244: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6760 - accuracy: 0.5719 - val_loss: 0.6936 - val_accuracy: 0.5387\n",
      "Epoch 245/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6743 - accuracy: 0.5722\n",
      "Epoch 00245: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6746 - accuracy: 0.5721 - val_loss: 0.6931 - val_accuracy: 0.5430\n",
      "Epoch 246/250\n",
      "84/88 [===========================>..] - ETA: 0s - loss: 0.6716 - accuracy: 0.5787\n",
      "Epoch 00246: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6712 - accuracy: 0.5789 - val_loss: 0.6948 - val_accuracy: 0.5344\n",
      "Epoch 247/250\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6760 - accuracy: 0.5694\n",
      "Epoch 00247: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6762 - accuracy: 0.5686 - val_loss: 0.6945 - val_accuracy: 0.5412\n",
      "Epoch 248/250\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6745 - accuracy: 0.5756\n",
      "Epoch 00248: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6745 - accuracy: 0.5756 - val_loss: 0.6937 - val_accuracy: 0.5380\n",
      "Epoch 249/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6730 - accuracy: 0.5774\n",
      "Epoch 00249: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.6734 - accuracy: 0.5764 - val_loss: 0.6954 - val_accuracy: 0.5334\n",
      "Epoch 250/250\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.6723 - accuracy: 0.5722\n",
      "Epoch 00250: val_accuracy did not improve from 0.54878\n",
      "88/88 [==============================] - 1s 14ms/step - loss: 0.6726 - accuracy: 0.5721 - val_loss: 0.6965 - val_accuracy: 0.5269\n",
      "Training completed in time:  0:05:06.370141\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "num_epochs = 250\n",
    "num_batch_size = 128\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath='mymodel2_{epoch:02d}.h5',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_accuracy` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1)\n",
    "]\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "breeding-elder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.6153570413589478\n",
      "Testing Accuracy:  0.5269010066986084\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "central-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test) # label scores \n",
    "\n",
    "classpreds = np.argmax(preds, axis=1) # predicted classes \n",
    "\n",
    "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
    "\n",
    "n_classes = 2 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "important-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "demographic-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_names = ['male', 'female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "lesbian-crystal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve for Each Class')\n",
    "for i in range(n_classes):\n",
    "    ax.plot(fpr[i], tpr[i], linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], c_names[i]))\n",
    "ax.legend(loc=\"best\", fontsize='x-large')\n",
    "ax.grid(alpha=.4)\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "statewide-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        male       0.55      0.77      0.64      1524\n",
      "      female       0.46      0.23      0.31      1264\n",
      "\n",
      "    accuracy                           0.53      2788\n",
      "   macro avg       0.50      0.50      0.47      2788\n",
      "weighted avg       0.51      0.53      0.49      2788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_testclass, classpreds, target_names=c_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-premium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
