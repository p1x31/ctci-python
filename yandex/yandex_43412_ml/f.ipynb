{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the binary neural network model\n",
    "class BinaryNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 128, kernel_size=9, padding=0)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(128, 128, kernel_size=9, padding=0)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=9, padding=0)\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = torch.sign(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = torch.sign(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = torch.sign(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sign(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   0      1\n",
       "1   1      1\n",
       "2   2      0\n",
       "3   3      1\n",
       "4   4      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to the directory containing the .npy files\n",
    "data_dir = \"/home/vadim/Documents/ctci-python/yandex/yandex_43412_ml/ml-intern-binary-biometry-contest/binary_train_data/binary_train\"\n",
    "\n",
    "# Set the path to the TSV file containing the labels\n",
    "labels_file = \"/home/vadim/Documents/ctci-python/yandex/yandex_43412_ml/ml-intern-binary-biometry-contest/binary_train.tsv\"\n",
    "\n",
    "# Load the filenames from the .npy files\n",
    "file_list = []\n",
    "\n",
    "for file_name in os.listdir(data_dir):\n",
    "    if file_name.endswith(\".npy\"):\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        file_list.append(file_path)\n",
    "\n",
    "# Load the labels from the TSV file\n",
    "labels_df = pd.read_csv(labels_file, header=None, usecols=[0,1], names=['id', 'label'], delimiter=\"\\t\")\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, file_list, label_list):\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        # Load the features from the file and return as a tensor\n",
    "        features = torch.from_numpy(np.load(file_path))\n",
    "        return features, label\n",
    "\n",
    "# Function to quantize and save the model\n",
    "def quantize_and_save_model(model, save_path):\n",
    "    with open(save_path, 'w') as fout:\n",
    "        for weights in model.parameters():\n",
    "            for param in torch.where(torch.flatten(weights) > 0, 1, -1):\n",
    "                fout.write(str(param.item()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels_df[\"label\"].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "    file_list, labels, test_size=0.2)\n",
    "\n",
    "# Save the split datasets as .npy files\n",
    "np.save(\"train_files.npy\", train_files)\n",
    "np.save(\"test_files.npy\", test_files)\n",
    "np.save(\"train_labels.npy\", train_labels)\n",
    "np.save(\"test_labels.npy\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 64])\n",
      "Input size: 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prepare the training and test data\n",
    "train_files = np.load(\"train_files.npy\")\n",
    "train_labels = np.load(\"train_labels.npy\")\n",
    "# split npy file into train and test\n",
    "\n",
    "test_files = np.load(\"test_files.npy\")\n",
    "test_labels = np.load(\"test_labels.npy\")\n",
    "\n",
    "\n",
    "train_dataset = GenderDataset(train_files, train_labels)\n",
    "test_dataset = GenderDataset(test_files, test_labels)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    " # Find the maximum length of feature tensors in both datasets\n",
    "max_length = max(len(features) for features, _ in train_dataset + test_dataset)\n",
    "\n",
    " # Function to pad the feature tensors\n",
    "def pad_features(features, length):\n",
    "    padded_features = F.pad(features, pad=(0, 0, 0, max_length - len(features)))\n",
    "    return padded_features\n",
    "\n",
    "# Apply padding to the feature tensors in the datasets\n",
    "train_dataset = [(pad_features(features, max_length), label) for features, label in train_dataset]\n",
    "test_dataset = [(pad_features(features, max_length), label) for features, label in test_dataset]\n",
    "\n",
    "for features, label in train_dataset:\n",
    "    print(features.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "# Set the batch size and number of epochs\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "input_size = train_dataset[0][0].shape[0]  # Get the input size from the first feature tensor\n",
    "print('Input size:', input_size)\n",
    "model = BinaryNetwork(input_size)\n",
    "#model = BinaryNetwork().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Accuracy: 16.0022\n",
      "Epoch [2/10], Accuracy: 16.0022\n",
      "Epoch [3/10], Accuracy: 16.0022\n",
      "Epoch [4/10], Accuracy: 16.0022\n",
      "Epoch [5/10], Accuracy: 16.0022\n",
      "Epoch [6/10], Accuracy: 16.0022\n",
      "Epoch [7/10], Accuracy: 16.0022\n",
      "Epoch [8/10], Accuracy: 16.0022\n",
      "Epoch [9/10], Accuracy: 16.0022\n",
      "Epoch [10/10], Accuracy: 16.0022\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        #batch_labels = batch_labels.type(torch.ByteTensor)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features.type(torch.FloatTensor))\n",
    "        loss = criterion(outputs, batch_labels.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            outputs = model(batch_features.type(torch.FloatTensor))\n",
    "            predicted = torch.round(torch.sigmoid(outputs)).long()\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print('Epoch [{}/{}], Accuracy: {:.4f}'.format(epoch+1, num_epochs, accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_and_save_model(model, 'model_weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_and_save_model(model, 'data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def remove_csv_header(input_file, output_file):\n",
    "    # Create a temporary file for writing the modified CSV data\n",
    "    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
    "\n",
    "    with open(input_file, 'r') as f_in, temp_file:\n",
    "        reader = csv.reader(f_in)\n",
    "        writer = csv.writer(temp_file)\n",
    "\n",
    "        # Skip the header row\n",
    "        next(reader)\n",
    "\n",
    "        # Write the remaining rows to the temporary file\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Replace the input file with the temporary file\n",
    "    shutil.move(temp_file.name, output_file)\n",
    "\n",
    "# Usage example\n",
    "input_file = 'data.csv'\n",
    "output_file = 'output.csv'\n",
    "remove_csv_header(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize and save the model\n",
    "quantize_and_save_model(model, 'data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_and_save_model(model, 'data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_and_save_model(model, save_path):\n",
    "    with open(save_path, 'w') as fout:\n",
    "        for layer in model.children():\n",
    "            if isinstance(layer, nn.Conv1d):\n",
    "                weights = layer.weight.detach().cpu().numpy()\n",
    "                bias = layer.bias.detach().cpu().numpy()\n",
    "                weights = np.where(weights > 0, 1, -1)\n",
    "                bias = np.where(bias > 0, 1, -1)\n",
    "                weights = weights.flatten()\n",
    "                bias = bias.flatten()\n",
    "                fout.write('\\n'.join(map(str, weights)))\n",
    "                fout.write('\\n')\n",
    "                fout.write('\\n'.join(map(str, bias)))\n",
    "                fout.write('\\n')\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                weights = layer.weight.detach().cpu().numpy()\n",
    "                bias = layer.bias.detach().cpu().numpy()\n",
    "                weights = np.where(weights > 0, 1, -1)\n",
    "                bias = np.where(bias > 0, 1, -1)\n",
    "                weights = weights.flatten()\n",
    "                bias = bias.flatten()\n",
    "                fout.write('\\n'.join(map(str, weights)))\n",
    "                fout.write('\\n')\n",
    "                fout.write('\\n'.join(map(str, bias)))\n",
    "                fout.write('\\n')\n",
    "quantize_and_save_model(model, 'data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def compress_csv(input_file, output_file):\n",
    "    with open(input_file, 'rt') as f_in:\n",
    "        with gzip.open(output_file, 'wt') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "\n",
    "# Usage example\n",
    "input_file = 'data.csv'\n",
    "output_file = 'data.csv.gz'\n",
    "compress_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "def cut_csv_by_half(input_file, output_file1, output_file2):\n",
    "    # Read the input CSV file\n",
    "    with open(input_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Determine the midpoint to divide the rows\n",
    "    midpoint = math.ceil(len(rows) / 2)\n",
    "\n",
    "    # Split the rows into two halves\n",
    "    rows1 = rows[:midpoint]\n",
    "    rows2 = rows[midpoint:]\n",
    "\n",
    "    # Write the first half to the first output CSV file\n",
    "    with open(output_file1, 'w', newline='') as file1:\n",
    "        writer1 = csv.writer(file1)\n",
    "        writer1.writerows(rows1)\n",
    "\n",
    "    # Write the second half to the second output CSV file\n",
    "    with open(output_file2, 'w', newline='') as file2:\n",
    "        writer2 = csv.writer(file2)\n",
    "        writer2.writerows(rows2)\n",
    "\n",
    "# Usage example\n",
    "input_file = 'output1.csv'\n",
    "output_file1 = 'output3.csv'\n",
    "output_file2 = 'output4.csv'\n",
    "cut_csv_by_half(input_file, output_file1, output_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def cut_csv_by_half(input_file, output_file1, output_file2):\n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Split the dataframe into two halves\n",
    "    midpoint = len(df) // 2\n",
    "    df1 = df.iloc[:midpoint-100000]\n",
    "    df2 = df.iloc[midpoint:]\n",
    "\n",
    "    # Write the first half to the first output CSV file\n",
    "    df1.to_csv(output_file1, index=False)\n",
    "\n",
    "    # Write the second half to the second output CSV file\n",
    "    df2.to_csv(output_file2, index=False)\n",
    "\n",
    "# Usage example\n",
    "input_file = 'data.csv'\n",
    "output_file1 = 'output1.csv'\n",
    "output_file2 = 'output2.csv'\n",
    "cut_csv_by_half(input_file, output_file1, output_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m input_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     26\u001b[0m output_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutput_chunks\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 27\u001b[0m split_csv_by_size(input_file, output_dir)\n",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m, in \u001b[0;36msplit_csv_by_size\u001b[0;34m(input_file, output_dir, chunk_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m i, chunk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunks):\n\u001b[1;32m     21\u001b[0m     output_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchunk_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     chunk\u001b[39m.\u001b[39;49mto_csv(output_file, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch112/lib/python3.9/site-packages/pandas/core/generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3770\u001b[0m )\n\u001b[0;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3773\u001b[0m     path_or_buf,\n\u001b[1;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3789\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch112/lib/python3.9/site-packages/pandas/io/formats/format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1185\u001b[0m )\n\u001b[0;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch112/lib/python3.9/site-packages/pandas/io/formats/csvs.py:259\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch112/lib/python3.9/site-packages/pandas/io/formats/csvs.py:264\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    263\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_header()\n\u001b[0;32m--> 264\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_body()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch112/lib/python3.9/site-packages/pandas/io/formats/csvs.py:302\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m start_i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m end_i:\n\u001b[1;32m    301\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_chunk(start_i, end_i)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch112/lib/python3.9/site-packages/pandas/io/formats/csvs.py:310\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    307\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc[slicer]\n\u001b[1;32m    309\u001b[0m res \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mto_native_types(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n\u001b[0;32m--> 310\u001b[0m data \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39miget_values(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(res\u001b[39m.\u001b[39mitems))]\n\u001b[1;32m    312\u001b[0m ix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_index[slicer]\u001b[39m.\u001b[39m_format_native_types(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n\u001b[1;32m    313\u001b[0m libwriters\u001b[39m.\u001b[39mwrite_csv_rows(\n\u001b[1;32m    314\u001b[0m     data,\n\u001b[1;32m    315\u001b[0m     ix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter,\n\u001b[1;32m    319\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch112/lib/python3.9/site-packages/pandas/io/formats/csvs.py:310\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    307\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc[slicer]\n\u001b[1;32m    309\u001b[0m res \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mto_native_types(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n\u001b[0;32m--> 310\u001b[0m data \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39;49miget_values(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(res\u001b[39m.\u001b[39mitems))]\n\u001b[1;32m    312\u001b[0m ix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_index[slicer]\u001b[39m.\u001b[39m_format_native_types(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n\u001b[1;32m    313\u001b[0m libwriters\u001b[39m.\u001b[39mwrite_csv_rows(\n\u001b[1;32m    314\u001b[0m     data,\n\u001b[1;32m    315\u001b[0m     ix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter,\n\u001b[1;32m    319\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch112/lib/python3.9/site-packages/pandas/core/internals/managers.py:1098\u001b[0m, in \u001b[0;36mBlockManager.iget_values\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     nb \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(block)(\n\u001b[1;32m   1094\u001b[0m         values, placement\u001b[39m=\u001b[39mbp, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, refs\u001b[39m=\u001b[39mblock\u001b[39m.\u001b[39mrefs \u001b[39mif\u001b[39;00m track_ref \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[1;32m   1096\u001b[0m     \u001b[39mreturn\u001b[39;00m SingleBlockManager(nb, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[\u001b[39m1\u001b[39m])\n\u001b[0;32m-> 1098\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39miget_values\u001b[39m(\u001b[39mself\u001b[39m, i: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[1;32m   1099\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39m    Return the data for column i as the values (ndarray or ExtensionArray).\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \n\u001b[1;32m   1102\u001b[0m \u001b[39m    Warning! The returned array is a view but doesn't handle Copy-on-Write,\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[39m    so this should be used with caution.\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m     \u001b[39m# TODO(CoW) making the arrays read-only might make this safer to use?\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def split_csv_by_size(input_file, output_dir, chunk_size=1):\n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Calculate the number of rows per chunk (approximately 1MB)\n",
    "    target_size = 1 * 1024 * 1024  # 1MB in bytes\n",
    "    num_rows = len(df)\n",
    "    rows_per_chunk = int(target_size / num_rows)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Split the dataframe into smaller chunks\n",
    "    chunks = [df[i:i + rows_per_chunk] for i in range(0, num_rows, rows_per_chunk)]\n",
    "\n",
    "    # Write each chunk to a separate CSV file\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_file = os.path.join(output_dir, f\"chunk_{i+1}.csv\")\n",
    "        chunk.to_csv(output_file, index=False)\n",
    "\n",
    "# Usage example\n",
    "input_file = 'data.csv'\n",
    "output_dir = 'output_chunks'\n",
    "split_csv_by_size(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
