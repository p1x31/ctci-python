{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "from pydub import AudioSegment\n",
    "# from pydub.playback import play\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "import functools\n",
    "import librosa, librosa.display\n",
    "print = functools.partial(print, flush=True) #this actually does make it a lot slower\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "churn_df = pd.read_csv(\"train/targets.tsv\")\n",
    "churn_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "import os\n",
    "import librosa, librosa.display\n",
    "import math\n",
    "from pydub import AudioSegment\n",
    "# from pydub.playback import play\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "import functools\n",
    "print = functools.partial(print, flush=True) #this actually does make it a lot slower\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "# os.chdir(dir_path)\n",
    "\n",
    "\n",
    "# signal, sr = librosa.load(file_path, sr=SAMPLE_RATE) # this is the soundfile method, which doesnt work with mp3/webm\n",
    "#check the samplerates = all webm youtube downloads have 48khz\n",
    "#pydub can open mp3/webm files and convert to samples/numpy arrays. thank you pydub\n",
    "#takes about 14 seconds to open 1hr long webm file, optimal way would be to just open chunks/buffers of it but oh well\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    #create accuracy subplot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"validation accuracy\")\n",
    "    axs[0].set_ylabel(\"accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "\n",
    "    #create loss subplot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train loss\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "    axs[1].set_ylabel(\"loss\")\n",
    "    axs[1].set_ylabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Loss\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mfcc_counter = 0\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "mlt = 2 #multiplier (yeah its weird)\n",
    "chunk = 1e3 * mlt  #1 second # 1e3 = 1000\n",
    "\n",
    "\n",
    "if 'mfcc_features.npy' in os.listdir():\n",
    "    print('loading saved data')\n",
    "    mfcc_features = load('mfcc_features.npy')\n",
    "    mfcc_labels = load('mfcc_labels.npy')\n",
    "else:\n",
    "\n",
    "    mfcc_features = np.empty((0,94*mlt,13))\n",
    "    mfcc_labels = []\n",
    "\n",
    "    for i, (root, dirs, filenames) in enumerate(os.walk(dir_path)):\n",
    "        for name in filenames:\n",
    "            if name.endswith('.webm'):\n",
    "\n",
    "                file_path = os.path.join(root, name)\n",
    "                print(file_path, i, file_path.split('\\\\')[-2])\n",
    "                sound = AudioSegment.from_file(file_path)\n",
    "                sound = sound.set_channels(1)\n",
    "                lenght_audio = len(sound)\n",
    "\n",
    "                number_mfccs = math.floor(lenght_audio/chunk)\n",
    "                print(lenght_audio/6e4,number_mfccs) #length in min, number chunks\n",
    "\n",
    "                mfcc_features_add = np.empty((number_mfccs,94*mlt,13))\n",
    "                mfcc_features = np.append(mfcc_features, mfcc_features_add, axis=0)\n",
    "\n",
    "                print(mfcc_features_add.shape)\n",
    "                print(mfcc_features.shape)\n",
    "\n",
    "                for x in range(number_mfccs):\n",
    "                    cut = sound[(x*chunk):(x*chunk)+chunk]\n",
    "                    samples = cut.get_array_of_samples()\n",
    "                    samples = np.array(samples)\n",
    "                    samples = samples.astype(float)\n",
    "\n",
    "                    # librosa.display.waveplot(samples, sr=sound.frame_rate)\n",
    "                    # plt.xlabel('Time')\n",
    "                    # plt.ylabel('Amplitude')\n",
    "                    # plt.show()\n",
    "\n",
    "                    mfcc = librosa.feature.mfcc(samples, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
    "                    mfcc = mfcc.T\n",
    "                    # librosa.display.specshow(mfcc, sr=44100, hop_length=hop_length)\n",
    "                    # plt.show()\n",
    "\n",
    "\n",
    "                    mfcc_features[mfcc_counter] = mfcc\n",
    "                    mfcc_labels.append(i-1)\n",
    "\n",
    "                    mfcc_counter += 1 \n",
    "\n",
    "                sound = None\n",
    "\n",
    "    mfcc_features = np.expand_dims(mfcc_features, axis=3)\n",
    "    save('mfcc_features.npy', mfcc_features)\n",
    "    mfcc_labels = np.array(mfcc_labels)\n",
    "    save('mfcc_labels.npy', mfcc_labels)\n",
    "\n",
    "print(mfcc_features.shape)\n",
    "\n",
    "output_neurons = mfcc_labels[-1] + 1\n",
    "print('output_neurons: ',output_neurons)\n",
    "\n",
    "\n",
    "#normalize??\n",
    "mean = np.mean(mfcc_features, axis=0)\n",
    "std = np.std(mfcc_features, axis=0)\n",
    "mfcc_features = (mfcc_features - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "#shuffle dataset ONCE\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(mfcc_features)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(mfcc_labels)\n",
    "np.random.seed()\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(mfcc_labels) * 0.9)\n",
    "val_size = len(mfcc_labels) - train_size\n",
    "\n",
    "print(train_size)\n",
    "print(val_size)\n",
    "\n",
    "training_ds = tf.data.Dataset.from_tensor_slices((mfcc_features,mfcc_labels))\n",
    "# training_ds = training_ds.shuffle(train_size + val_size)\n",
    "\n",
    "\n",
    "val_ds = training_ds.skip(train_size).take(val_size)\n",
    "training_ds = training_ds.take(train_size)\n",
    "\n",
    "print(val_ds)\n",
    "print(training_ds)\n",
    "\n",
    "\n",
    "#these two do the same thing, neat\n",
    "# print(next(iter(training_ds.take(1)))[1])\n",
    "# for item in training_ds.take(1):\n",
    "# \tprint(item[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################build model##############\n",
    "\n",
    "# batch_size = np_samples.shape[0]\n",
    "batch_size = 32 * 1\n",
    "STEPS_PER_EPOCH = train_size//batch_size\n",
    "checkpoint_path = 'savedweights_1/chkp-{val_accuracy}.ckpt'\n",
    "\n",
    "def get_callbacks():\n",
    "  return [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10), #this will stop the training if there is no improvement since the last 10 epochs\n",
    "    # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_weights_only=True, period=1) #unmute if you want to save ur weights during training\n",
    "  ]\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "\n",
    "inputShape = (94*mlt, 13, 1)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "\n",
    "    #1st conv layer\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(94*mlt, 13, 1), kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    #2nd conv layer\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.001)), #?? does it not need input layers here?\n",
    "    tf.keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    # #3rd conv layer\n",
    "    tf.keras.layers.Conv2D(32, (2,2), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPool2D((2,2), strides=(2,2), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    #flatten & dense & output\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(output_neurons)\n",
    "    ])\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              # loss=loss_fn,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# weight_path = '../savedweights_1/chkp-0.9171686768531799.ckpt'   \n",
    "# model.load_weights(weight_path)  #load them and comments out model.fit if u just want to use pre-existing weights\n",
    "\n",
    "# train\n",
    "history = model.fit(training_ds.batch(batch_size), \t\n",
    "    epochs=40, \n",
    "    validation_data=val_ds.shuffle(val_size).batch(batch_size),\n",
    "    callbacks=get_callbacks(),\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "\n",
    "model.evaluate(val_ds.batch(batch_size), verbose=2)\n",
    "\n",
    "\n",
    "#plot accuracy and error over the epochs\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "\n",
    "test_file = 'skrillex scary.webm'\n",
    "sound = AudioSegment.from_file(test_file)\n",
    "sound = sound.set_channels(1)\n",
    "lenght_audio = len(sound)\n",
    "\n",
    "number_mfccs = math.floor(lenght_audio/chunk)\n",
    "print(lenght_audio/6e4,number_mfccs) #length in min, number chunks\n",
    "\n",
    "test_mfccs = np.empty((number_mfccs,94*mlt,13))\n",
    "\n",
    "for x in range(number_mfccs):\n",
    "    cut = sound[(x*chunk):(x*chunk)+chunk]\n",
    "    samples = cut.get_array_of_samples()\n",
    "    samples = np.array(samples)\n",
    "    samples = samples.astype(float)\n",
    "    mfcc = librosa.feature.mfcc(samples, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
    "    mfcc = mfcc.T\n",
    "    # librosa.display.specshow(mfcc, sr=44100, hop_length=hop_length)\n",
    "    # plt.show()\n",
    "    test_mfccs[x] = mfcc\n",
    "\n",
    "# test_mfccs = np.expand_dims(test_mfccs, axis=0)\n",
    "test_mfccs = np.expand_dims(test_mfccs, axis=3)\n",
    "print(test_mfccs.shape)\n",
    "\n",
    "#normalize\n",
    "mean = np.mean(test_mfccs, axis=0)\n",
    "std = np.std(test_mfccs, axis=0)\n",
    "test_mfccs = (test_mfccs - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "class_names = ['ambient', 'classical', 'dubstep', 'hardstyle', 'jazz', 'trance']\n",
    "prob_results = []\n",
    "\n",
    "for i in range(len(test_mfccs)):\n",
    "    result = probability_model(test_mfccs[i:i+1]) #because it needs to be a list in a list, [i,4410] just returns a 1d list\n",
    "    answer = np.argmax(result[0])\n",
    "    prob_results.append(answer) \n",
    "    print(\"Skrillex is a \" + class_names[answer])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "yhat = predict(X_test)\n",
    "\n",
    "df['tgender'] = yhat\n",
    "df.to_csv('targets.tsv',index=False)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc8fab90f86921a490c15b33fc48436930ad3fb5a513ca112898407c8a5a09c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
