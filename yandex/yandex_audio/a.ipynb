{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets, load_planar_dataset1\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "m = shape_Y[1]  # training set size\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression classifier\n",
    "clf = sklearn.linear_model.LogisticRegressionCV();\n",
    "clf.fit(X.T, Y.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary for logistic regression\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
    "plt.title(\"Logistic Regression\")\n",
    "\n",
    "# Print accuracy\n",
    "LR_predictions = clf.predict(X.T)\n",
    "print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n",
    "       '% ' + \"(percentage of correctly labelled datapoints)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess = layer_sizes_test_case()\n",
    "(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    # \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = initialize_parameters_test_case()\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, parameters = forward_propagation_test_case()\n",
    "\n",
    "A2, cache = forward_propagation(X_assess, parameters)\n",
    "\n",
    "# Note: we use the mean here just to make sure that your output matches ours. \n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "    \n",
    "    # Retrieve W1 and W2 from parameters\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Compute the cross-entropy cost\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    logprobs = np.multiply(np.log(A2),Y)\n",
    "    cost = - np.sum(np.multiply(np.log(A2), Y) + np.multiply(np.log(1. - A2), 1. - Y)) / m\n",
    "    #cost = np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))/(-m)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2, Y_assess, parameters = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m \n",
    "    db2 = np.sum(dZ2, axis = 1, keepdims = True) / m\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2)\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
    "\n",
    "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    import pdb\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess = nn_model_test_case()\n",
    "\n",
    "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "#     Y_prediction = np.zeros((1, X.shape[1]))\n",
    "    predictions = np.array([0 if i <= 0.5 else 1 for i in np.squeeze(A2)])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, X_assess = predict_test_case()\n",
    "\n",
    "predictions = predict(parameters, X_assess)\n",
    "print(\"predictions mean = \" + str(np.mean(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take about 2 minutes to run\n",
    "\n",
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n",
    "\n",
    "datasets = {\"noisy_circles\": noisy_circles,\n",
    "            \"noisy_moons\": noisy_moons,\n",
    "            \"blobs\": blobs,\n",
    "            \"gaussian_quantiles\": gaussian_quantiles}\n",
    "\n",
    "### START CODE HERE ### (choose your dataset)\n",
    "dataset = \"gaussian_quantiles\"\n",
    "### END CODE HERE ###\n",
    "\n",
    "X, Y = datasets[dataset]\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "\n",
    "# make blobs binary\n",
    "if dataset == \"blobs\":\n",
    "    Y = Y%2\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets, load_planar_dataset1\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate1():  \n",
    "    a = uniform(0, 1)  \n",
    "    b = uniform(0, 1)  \n",
    "    return (a * cos(2 * pi * b), a * sin(2 * pi * b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate2():  \n",
    "    while True:  \n",
    "        x = uniform(-1, 1)  \n",
    "        y = uniform(-1, 1)  \n",
    "        if x ** 2 + y ** 2 > 1:  \n",
    "            continue  \n",
    "        return (x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
