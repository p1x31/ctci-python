{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fb80f019e8f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import array\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "import re\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "import librosa, librosa.display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "    class_names = [\"kick\",\"snare\",\"clap\",\"hihat\"] \n",
    "\n",
    "    train_path_list = [\"train_kicks\\\\\",\"train_snares\\\\\",\"train_claps\\\\\",\"train_hats\\\\\"]\n",
    "    test_path_list = [\"test_kicks\\\\\",\"test_snares\\\\\",\"test_claps\\\\\",\"test_hats\\\\\"]\n",
    "\n",
    "    arr = []\n",
    "    for a in train_path_list:\n",
    "    for i in os.listdir(a):\n",
    "        arr.append(a  + i)\n",
    "\n",
    "\n",
    "    print(len(arr))\n",
    "\n",
    "    sample_list = []\n",
    "\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "\n",
    "\n",
    "    def pitchSample(octaves,sound):\n",
    "    new_sample_rate = int(sound.frame_rate * (2.0 ** octaves))\n",
    "    hipitch_sound = sound._spawn(sound.raw_data, overrides={'frame_rate': new_sample_rate})\n",
    "    hipitch_sound = hipitch_sound.set_frame_rate(44100)\n",
    "    return hipitch_sound\n",
    "\n",
    "    def bassBoostSample(cutoff,sound):\n",
    "    #no actually parametric eq so i layer a lp over\n",
    "    print(\"cutoff: \",cutoff*10)\n",
    "    lowpassed = AudioSegment.low_pass_filter(sound,(cutoff*10)+100)\n",
    "    augmented_sound = sound + lowpassed\n",
    "    return augmented_sound\n",
    "\n",
    "    def ms_samples(sample_length):\n",
    "    return int((44100 / 1000) * sample_length)\n",
    "\n",
    "\n",
    "    def augmentor(sound,aug):\n",
    "    if aug == 1:\n",
    "        pitched_sounds.append(sound)\n",
    "    else:\n",
    "        for i in range(aug):\n",
    "            aug = float(aug)\n",
    "            spread = ((aug/100) - (aug*2)/100) + (aug/100)*i\n",
    "            pitched_sounds.append(pitchSample(spread,sound))\n",
    "            # pitched_sounds.append(bassBoostSample(spread,sound))\n",
    "    return pitched_sounds\n",
    "\n",
    "\n",
    "    sample_length = 100 #this is ms!\n",
    "    aug = 9\n",
    "\n",
    "\n",
    "\n",
    "    amount_entries = len(arr)*aug\n",
    "    np_mfcc = np.empty((amount_entries, 9, 13))\n",
    "\n",
    "    #if you already have mfcc's saved it will just load them / if you want to create new ones delete them from the file\n",
    "    if 'np_mfcc.npy' in os.listdir():\n",
    "    print('loading saved data')\n",
    "    np_mfcc = load('np_mfcc.npy')\n",
    "    sample_list = load('sample_list.npy')\n",
    "    else:\n",
    "    for i in range(len(arr)): #create mfcc\n",
    "\n",
    "        # cant figure out how to play 32bit file\n",
    "        sound = AudioSegment.from_file(arr[i], format=\"wav\", channels=1)\n",
    "        sound = sound.set_channels(1)\n",
    "\n",
    "        pitched_sounds = []\n",
    "        augmentor(sound,aug)\n",
    "\n",
    "\n",
    "        for x in range(len(pitched_sounds)):\n",
    "\n",
    "\n",
    "            if re.search(\"kicks\",arr[i]):\n",
    "                sample_list.append(0)\n",
    "            elif re.search(\"snares\",arr[i]):\n",
    "                sample_list.append(1)\n",
    "            elif re.search(\"clap\",arr[i]):\t\n",
    "                sample_list.append(2)\n",
    "            else:\t\n",
    "                sample_list.append(3)\n",
    "\n",
    "\n",
    "            sound = pitched_sounds[x][:sample_length]\n",
    "\n",
    "            samples = sound.get_array_of_samples()\n",
    "\n",
    "\n",
    "            if len(samples) < ms_samples(sample_length):\n",
    "                padding_samples = ms_samples(sample_length) - len(samples) \n",
    "                for dumi in range(padding_samples):\n",
    "                    samples.append(0)\n",
    "\n",
    "\n",
    "            #turned audio segment into mfcc\n",
    "            samples = np.array(samples)\n",
    "            samples = samples.astype(float)\n",
    "            mfcc = librosa.feature.mfcc(samples, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
    "            mfcc = mfcc.T\n",
    "            # librosa.display.specshow(mfcc, sr=44100, hop_length=hop_length)\n",
    "            # plt.show()\n",
    "            mfcc = np.expand_dims(mfcc, axis=0)\n",
    "\n",
    "            # np_mfcc = np.append(np_mfcc, mfcc, axis=2)\n",
    "            np_mfcc[i*aug+x] = mfcc\n",
    "\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(np.floor((i*100)/len(arr)))\n",
    "    save('np_mfcc.npy', np_mfcc)\n",
    "    sample_list = np.array(sample_list)\n",
    "    save('sample_list.npy', sample_list)\n",
    "\n",
    "    #you need to have a 3d shape for a conv2D layer, 4d if you include batch size\n",
    "    np_mfcc = np.expand_dims(np_mfcc, axis=3)\n",
    "\n",
    "\n",
    "    #shuffle ONCE\n",
    "    seed = 10\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(np_mfcc)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(sample_list)\n",
    "    np.random.seed()\n",
    "\n",
    "\n",
    "    #new test/validation samples\n",
    "    arr = []\n",
    "    for a in test_path_list:\n",
    "    for i in os.listdir(a):\n",
    "        arr.append(a  + i)\n",
    "\n",
    "\n",
    "    test_sample_list = []\n",
    "    # test_np_samples = np.empty((len(arr),ms_samples(sample_length)))\n",
    "    test_np_mfcc = np.empty((len(arr), 9, 13))\n",
    "\n",
    "    for i in range(len(arr)):\n",
    "\n",
    "    # cant figure out how to play 32bit file\n",
    "    sound = AudioSegment.from_file(arr[i], format=\"wav\", channels=1)\n",
    "    sound = sound.set_channels(1)\n",
    "\n",
    "    sound = sound[:sample_length]\n",
    "    samples = sound.get_array_of_samples()\n",
    "\n",
    "    if re.search(\"kicks\",arr[i]):\n",
    "        test_sample_list.append(0)\n",
    "    elif re.search(\"snares\",arr[i]):\n",
    "        test_sample_list.append(1)\n",
    "    elif re.search(\"claps\",arr[i]):\n",
    "        test_sample_list.append(2)\n",
    "    else:\n",
    "        test_sample_list.append(3)\n",
    "\n",
    "\n",
    "    if len(samples) < ms_samples(sample_length):\n",
    "        padding_samples = ms_samples(sample_length) - len(samples) \n",
    "        for padno in range(padding_samples):\n",
    "            samples.append(0)\n",
    "\n",
    "    # test_np_samples[i] = samples\n",
    "    samples = np.array(samples)\n",
    "    samples = samples.astype(float)\n",
    "    mfcc = librosa.feature.mfcc(samples, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
    "    mfcc = mfcc.T\n",
    "\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "    test_np_mfcc[i] = mfcc\n",
    "\n",
    "\n",
    "    test_sample_list = np.array(test_sample_list)\n",
    "    test_np_mfcc = np.expand_dims(test_np_mfcc, axis=3)\n",
    "    # test_np_samples = test_np_samples.astype(float) / (2**15)\n",
    "\n",
    "\n",
    "    #shuffle ONCE\n",
    "    seed = 10\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(test_sample_list)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(test_np_mfcc)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(arr)\n",
    "    np.random.seed()\n",
    "\n",
    "\n",
    "    train_size = int(amount_entries * 0.9)\n",
    "    val_size = amount_entries - train_size\n",
    "\n",
    "    training_ds = tf.data.Dataset.from_tensor_slices((np_mfcc,sample_list))\n",
    "    # dataset = training_ds.shuffle(train_size + val_size)\n",
    "\n",
    "\n",
    "    val_ds = training_ds.skip(train_size).take(val_size)\n",
    "    training_ds = training_ds.take(train_size)\n",
    "\n",
    "    print(val_ds)\n",
    "    print(training_ds)\n",
    "\n",
    "\n",
    "\n",
    "    ####################build model##############\n",
    "\n",
    "    # batch_size = np_samples.shape[0]\n",
    "    batch_size = 320\n",
    "    STEPS_PER_EPOCH = train_size//batch_size\n",
    "\n",
    "    def get_callbacks():\n",
    "    return [\n",
    "    # tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6),\n",
    "    # tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "    ]\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "\n",
    "    def get_optimizer():\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "\n",
    "\n",
    "    inputShape = (9, 13, 1)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "\n",
    "    #1st conv layer\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(9, 13, 1), kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    #2nd conv layer\n",
    "    # tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=regularizers.l2(0.001)), #?? does it not need input layers here?\n",
    "    # tf.keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    # #3rd conv layer\n",
    "    # tf.keras.layers.Conv2D(32, (2,2), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    # tf.keras.layers.MaxPool2D((2,2), strides=(2,2), padding='same'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    #flatten & dense & output\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(len(train_path_list))\n",
    "    ])\n",
    "\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    history = model.fit(training_ds.shuffle(train_size).batch(batch_size), \t\n",
    "    epochs=40, \n",
    "    validation_data=val_ds.batch(batch_size),\n",
    "    # validation_steps=val_size, \n",
    "    callbacks=get_callbacks(),\n",
    "    verbose=1,\n",
    "    # batch_size=32)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    model.evaluate(test_np_mfcc, test_sample_list, verbose=2)\n",
    "\n",
    "    #history history plot stuff\n",
    "    # history_dict = history.history\n",
    "\n",
    "    # acc = history_dict['accuracy']\n",
    "    # val_acc = history_dict['val_accuracy']\n",
    "    # loss = history_dict['loss']\n",
    "    # val_loss = history_dict['val_loss']\n",
    "\n",
    "    # epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # # \"bo\" is for \"blue dot\"\n",
    "    # plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # # b is for \"solid blue line\"\n",
    "    # plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    # plt.title('Training and validation loss')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # plt.clf()   # clear figure\n",
    "\n",
    "    # plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    # plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    # plt.title('Training and validation accuracy')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Accuracy')\n",
    "    # plt.legend(loc='lower right')\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    probability_model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "\n",
    "    for i in range(len(test_np_mfcc)):\n",
    "    result = probability_model(test_np_mfcc[i:i+1]) #because it needs to be a list in a list, [i,4410] just returns a 1d list\n",
    "    answer = np.argmax(result[0])    \n",
    "    print(arr[i] + \" is a \" + class_names[answer])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # plt.imshow(img, cmap=plt.cm.binary)\n",
    "    plt.plot(img)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "    else:\n",
    "    color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "\n",
    "    def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(4))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(4), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
